{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install python-dotenv langgraph langchain langchain-community langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# api key를 포함한 환경변수 세팅\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 답변에 필요한 정보를 모아서 벡터DB를 구성한다.\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# LLM 모델 생성\n",
    "model = ChatOpenAI(temperature=0, model=os.getenv(\"OPENAI_API_MODEL\"), streaming=True)\n",
    "\n",
    "embedding = OpenAIEmbeddings(base_url=os.getenv(\"OPENAI_EMBEDDINGS_API_BASE\"),\n",
    "                              api_key=os.getenv(\"OPENAI_EMBEDDINGS_API_KEY\"), \n",
    "                              model=os.getenv(\"OPENAI_EMBEDDINGS_API_MODEL\"))\n",
    "\n",
    "# %%\n",
    "\n",
    "urls = [\n",
    "    # \"https://www.google.com/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1500, chunk_overlap=100\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "client_settings = Settings(\n",
    "    #chroma_api_impl=\"rest\",\n",
    "    chroma_server_host=\"localhost\",\n",
    "    #chroma_server_ssl_verify=False,\n",
    "    chroma_server_http_port=8000\n",
    ")\n",
    "\n",
    "# 벡터 데이터베이스에 문서 추가\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    client_settings=client_settings,\n",
    "    embedding=embedding,\n",
    "    collection_name=\"langgrah-test-db\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 툴 정의\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# 벡터DB의 정보를 얻기 위한 리트리버 생성\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 릴리안 웡의 블로그 게시물에 대한 정보를 검색하고 반환하는 도구를 생성\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n",
    ")\n",
    "\n",
    "tools = [tool]\n",
    "\n",
    "# 도구들을 실행할 ToolExecutor 객체를 생성\n",
    "tool_executor = ToolNode(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 그래프 요소들을 정의 (Edge, Node)\n",
    "import json\n",
    "import operator\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# 그래프 상태\n",
    "class AgentState(TypedDict):\n",
    "    # 메시지 시퀀스\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Edges\n",
    "\n",
    "def should_retrieve(state):\n",
    "    \"\"\"\n",
    "    에이전트가 더 많은 정보를 검색해야 하는지 또는 프로세스를 종료해야 하는지 결정\n",
    "\n",
    "    이 함수는 상태의 마지막 메시지에서 함수 호출을 확인 함수 호출이 있으면 정보 검색 프로세스를 계속 그렇지 않으면 프로세스를 종료\n",
    "\n",
    "    Args:\n",
    "        state (messages): 현재 상태\n",
    "\n",
    "    Returns:\n",
    "        str: 검색 프로세스를 \"계속\"하거나 \"종료\"하는 결정\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO RETRIEVE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # 함수 호출이 없으면 종료\n",
    "    if \"tool_calls\" not in last_message.additional_kwargs:\n",
    "        print(\"---DECISION: DO NOT RETRIEVE / DONE---\")\n",
    "        return \"end\"\n",
    "    # 그렇지 않으면 함수 호출이 있으므로 계속\n",
    "    else:\n",
    "        print(\"---DECISION: RETRIEVE---\")\n",
    "        return \"continue\"\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    검색된 문서가 질문과 관련이 있는지 여부를 결정\n",
    "\n",
    "    Args:\n",
    "        state (messages): 현재 상태\n",
    "\n",
    "    Returns:\n",
    "        str: 문서가 관련이 있는지 여부에 대한 결정\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # 데이터 모델\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"관련성 검사를 위한 이진 점수.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"'yes' 또는 'no'의 관련성 점수\")\n",
    "\n",
    "    # 도구\n",
    "    grade_tool_oai = convert_to_openai_tool(grade)\n",
    "\n",
    "    # 도구와 강제 호출을 사용한 LLM\n",
    "    llm_with_tool = model.bind(\n",
    "        tools=[convert_to_openai_tool(grade_tool_oai)],\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}},\n",
    "    )\n",
    "\n",
    "    # 파서\n",
    "    parser_tool = PydanticToolsParser(tools=[grade])\n",
    "\n",
    "    # 프롬프트\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # 체인\n",
    "    chain = prompt | llm_with_tool | parser_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    score = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    grade = score[0].binary_score\n",
    "\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(grade)\n",
    "        return \"no\"\n",
    "\n",
    "# Nodes\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    현재 상태를 기반으로 에이전트 모델을 호출하여 응답을 생성 질문에 따라 검색 도구를 사용하여 검색을 결정하거나 단순히 종료\n",
    "\n",
    "    Args:\n",
    "        state (messages): 현재 상태\n",
    "\n",
    "    Returns:\n",
    "        dict: 메시지에 에이전트 응답이 추가된 업데이트된 상태\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    # model = ChatOpenAI(temperature=0, streaming=True,\n",
    "    #                    model=\"gpt-4o\")\n",
    "    # 현재 사용 가능한 tool들을 모두 openai용 함수 형식으로 바꾸어 첨부한다.\n",
    "    functions = [convert_to_openai_tool(t) for t in tools]\n",
    "    llm_with_func = model.bind_tools(functions)    \n",
    "    response = llm_with_func.invoke(messages)\n",
    "    # 이것은 기존 목록에 추가될 것이므로 리스트를 반환.\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    도구를 사용하여 검색을 실행\n",
    "\n",
    "    Args:\n",
    "        state (messages): 현재 상태\n",
    "\n",
    "    Returns:\n",
    "        dict: 검색된 문서가 추가된 업데이트된 상태\n",
    "    \"\"\"\n",
    "    print(\"---EXECUTE RETRIEVAL---\")\n",
    "    messages = state[\"messages\"]\n",
    "    # 계속 조건을 기반으로 마지막 메시지가 함수 호출을 포함하고 있음을 알 수 있습니다.\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    tool_calls = last_message.additional_kwargs[\"tool_calls\"]\n",
    "    if len(tool_calls) > 0:\n",
    "        tool_call = tool_calls[0]\n",
    "        tool_name = tool_call[\"function\"][\"name\"]\n",
    "        tool_input = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "\n",
    "        tool = tool_executor.tools_by_name[tool_name]\n",
    "        observation = tool.invoke(tool_input)\n",
    "        function_message = ToolMessage(content=observation, tool_call_id=tool_call[\"id\"])\n",
    "\n",
    "        # 이것은 기존 목록에 추가될 것이므로 리스트를 반환\n",
    "        return {\"messages\": [function_message]}\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    질문을 변형하여 더 나은 질문을 생성\n",
    "\n",
    "    Args:\n",
    "        state (messages): 현재 상태\n",
    "\n",
    "    Returns:\n",
    "        dict: 재구성된 질문이 추가된 업데이트된 상태\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 평가자\n",
    "    # model = ChatOpenAI(\n",
    "    #     temperature=0, model=\"gpt-4o\", streaming=True)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    답변 생성\n",
    "\n",
    "    Args:\n",
    "        state (messages): 현재 상태\n",
    "\n",
    "    Returns:\n",
    "         dict: 재구성된 질문이 추가된 업데이트된 상태\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    # 프롬프트\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    # model = ChatOpenAI(model_name=\"gpt-4o\",\n",
    "    #                  temperature=0, streaming=True)\n",
    "\n",
    "    # 후처리\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # 체인\n",
    "    rag_chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # 실행\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 그래프 생성\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# 순환할 노드들을 정의\n",
    "workflow.add_node(\"agent\", agent)  # 에이전트 노드를 추가\n",
    "workflow.add_node(\"retrieve\", retrieve)  # 정보 검색 노드를 추가\n",
    "workflow.add_node(\"rewrite\", rewrite)  # 정보 재작성 노드를 추가\n",
    "workflow.add_node(\"generate\", generate)  # 정보 생성 노드를 추가\n",
    "\n",
    "# 에이전트 노드 호출하여 검색 여부 결정\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# 검색 여부 결정\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # 에이전트 결정 평가\n",
    "    should_retrieve,\n",
    "    {\n",
    "        # 도구 노드 호출\n",
    "        \"continue\": \"retrieve\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# `action` 노드 호출 후 진행될 경로\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # 에이전트 결정 평가\n",
    "    grade_documents,\n",
    "    {\n",
    "        \"yes\": \"generate\",\n",
    "        \"no\": \"rewrite\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# 컴파일\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 그래프를 출력한다. (Jupyter Notebook에서만 유효)\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 그래프를 이용하여 질의\n",
    "import pprint\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# HumanMessage 객체를 사용하여 질문 메시지를 정의\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"What does Lilian Weng say about the types of agent memory?\"\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "# app.stream을 통해 입력된 메시지에 대한 출력을 스트리밍\n",
    "for output in app.stream(inputs):\n",
    "    # 출력된 결과에서 키와 값을 순회\n",
    "    for key, value in output.items():\n",
    "        # 노드의 이름과 해당 노드에서 나온 출력을 출력\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        # 출력 값을 예쁘게 출력\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    # 각 출력 사이에 구분선을 추가\n",
    "    pprint.pprint(\"\\n---\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
